{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "liberal-compilation",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "needed-thursday",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T20:49:53.138691Z",
     "start_time": "2021-03-11T20:49:52.104374Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import os\n",
    "\n",
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-mediterranean",
   "metadata": {},
   "source": [
    "## Get a list of all the HTML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fatal-iceland",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T20:49:53.146970Z",
     "start_time": "2021-03-11T20:49:53.141360Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['exxon_mobil_html.csv',\n",
       " 'walmart_html.csv',\n",
       " 'amazon_html.csv',\n",
       " 'cvs_health_html.csv',\n",
       " 'apple_html.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir('./html/')\n",
    "html_files = [f for f in files if 'html' in f]\n",
    "html_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-interim",
   "metadata": {},
   "source": [
    "### Get all the links from each block of html in each of the files "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-evolution",
   "metadata": {},
   "source": [
    "For each file in the html folder, parse the HTML in each row to gather all of the links from the webpage. These files are saved in the links folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "nuclear-robinson",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T20:50:07.442354Z",
     "start_time": "2021-03-11T20:49:53.149020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exxon_mobil_links.csv\n",
      "walmart_links.csv\n",
      "amazon_links.csv\n",
      "cvs_health_links.csv\n",
      "apple_links.csv\n"
     ]
    }
   ],
   "source": [
    "# iterate through all the files in the folder\n",
    "for file in html_files:\n",
    "\n",
    "    # create a new file name for later\n",
    "    new_file_name = file.replace('html.csv', 'links.csv')\n",
    "    link_files = os.listdir('./links/')\n",
    "    \n",
    "    if new_file_name in link_files:\n",
    "        pass\n",
    "    else:\n",
    "        print(new_file_name)\n",
    "    # create a dataframe from the csv file\n",
    "        df = pd.read_csv(f'./html/{file}')\n",
    "\n",
    "    # start a list that information can be appended to\n",
    "        hrefs = []\n",
    "\n",
    "        for row in range(len(df)):\n",
    "\n",
    "            # get html from each row of the dataframe\n",
    "            html = df.loc[row, 'html']\n",
    "\n",
    "            # use BeautifulSoup to read in the html\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "            # get a list of all the tags on that page\n",
    "            tags = set([tag.name for tag in soup.find_all()])\n",
    "\n",
    "            # iterate through all of the possible tags to get\n",
    "            # all of the links, regardless of how it is tagged\n",
    "            for tag in tags:\n",
    "\n",
    "                # iterate through all of the elements with that tag\n",
    "                for element in soup.find_all(tag):\n",
    "                    # open a dictionary to append to the hrefs list\n",
    "                    href = {}\n",
    "                    # only append if the element has a link\n",
    "                    if 'href' in element.attrs:\n",
    "                        href['company'] = df.loc[row, 'company']\n",
    "                        href['newsroom_page'] = row\n",
    "                        href['tag'] = tag\n",
    "                        href['link'] = element.attrs['href']\n",
    "                        hrefs.append(href)\n",
    "\n",
    "        links = pd.DataFrame(hrefs).drop_duplicates(subset=['link']).dropna()\n",
    "        links.to_csv(f'./links/{new_file_name}', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-administration",
   "metadata": {},
   "source": [
    "## Editing `links` files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-transmission",
   "metadata": {},
   "source": [
    "Due to the volume of unnecessary/irrelevant links obtained by just grabbing all of the HTML from the webpage, the below filters out anything that *isn't* a link to a press release using a loop and saving the filtered DataFrame back to the links file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rubber-magic",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T20:50:07.449656Z",
     "start_time": "2021-03-11T20:50:07.444114Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple_links.csv',\n",
       " 'walmart_links.csv',\n",
       " 'cvs_health_links.csv',\n",
       " 'amazon_links.csv',\n",
       " 'exxon_mobil_links.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_files = [f for f in os.listdir('./links/') if 'csv' in f]\n",
    "link_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mental-berry",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T20:50:07.455467Z",
     "start_time": "2021-03-11T20:50:07.451768Z"
    }
   },
   "outputs": [],
   "source": [
    "pr_links_dict = {\n",
    "    'Apple': {\n",
    "        'link': '/newsroom/20(21|20|19)',\n",
    "        'base': 'https://www.apple.com'\n",
    "    },\n",
    "    'Walmart': {\n",
    "        'link': 'https://corporate.walmart.com/newsroom/20(21|20|19)',\n",
    "        'base': ''\n",
    "    },\n",
    "    'CVS Health': {\n",
    "        'link': '/(newsroom|news-and-insights)/press-releases/',\n",
    "        'base': 'https://www.cvshealth.com'\n",
    "    },\n",
    "    'Amazon': {\n",
    "        'link': '/news-releases/news-release-details/',\n",
    "        'base': 'https://press.aboutamazon.com'\n",
    "    },\n",
    "    'Exxon Mobil': {\n",
    "        'link': '/News/Newsroom/News-releases/20(21|20|19)',\n",
    "        'base': 'https://corporate.exxonmobil.com'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "damaged-russia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T20:50:07.537362Z",
     "start_time": "2021-03-11T20:50:07.457670Z"
    }
   },
   "outputs": [],
   "source": [
    "# filter the links based on the filter string and add the base url\n",
    "for file in link_files:\n",
    "    df = pd.read_csv(f'./links/{file}').dropna()\n",
    "    company = df.company[0]\n",
    "    filter_string = pr_links_dict[company]['link']\n",
    "    df = df[df['link'].str.contains(filter_string)]\n",
    "    df['base'] = pr_links_dict[company]['base']\n",
    "    df.to_csv(f'./links/{file}', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
